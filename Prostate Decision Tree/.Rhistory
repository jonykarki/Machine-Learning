Dat = read.spss("C:\\Users\\Dark Matter\\OneDrive - The University of Texas at Tyler\\Research\\W1\\Sustainability_ social_ output_2_3_2020.spv")
library(MASS)
# ISLR has to be installed separately
install.packages("ISLR")
library(ISLR)
?Boston
fix(Boston)
View(Boston)
names(Boston)
lm.fit = lm(medv~lstat)
lm.fit = lm(medv~lstat, data=Boston)
fix(lm.fit)
View(lm.fit)
# if we do attach(Boston)
attach(Boston)
# now lm recognizes the variables
lm.fit = lm(medv ~ lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
data()
ChickWeight
model = lm(Weight~Time, data=ChickWeight)
model = lm(ChickWeight$Weight~ChickWeight$Time, data=ChickWeight)
model = lm(ChickWeight$weight~ChickWeight$Time, data=ChickWeight)
print(model)
summary(model)
rm(list=ls())
data(Glass)
library(textir)
install.packages("textir")
library(textir)
library(MASS)
data(Glass)
data(fgl)
fgl
View(fgl)
#set the grid size
par(mfrow=c(3,3), mai(.3,.6,.1,.1))
#set the grid size
par(mfrow=c(3,3), mai=c(.3,.6,.1,.1))
plot(RI ~ type, data=fgl)
plot(RI ~ type, data=fgl, col=c(grey(.2),2:6))
plot(RI ~ type, data=fgl, col=c(grey(.2),2:6))
#set the grid size
par(mfrow=c(3,3), mai=c(.3,.6,.1,.1))
plot(RI ~ type, data=fgl, col=c(grey(.2),2:6))
plot(RI ~ type, data=fgl, col=c(grey(.2),3:6))
plot(RI ~ type, data=fgl, col=c(grey(.2),2:6))
#set the grid size
par(mfrow=c(3,3), mai=c(.3,.6,.1,.1))
#set the grid size
par(mfrow=c(3,3), mai=c(.3,.6,.1,.1))
plot(RI ~ type, data=fgl, col=c(grey(.2),2:6))
plot(AI ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Al ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Na ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Ba ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Mg ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Si ~ type, data=fgl, col=c(grey(.2),2:6))
plot(K ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Ca ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Fe ~ type, data=fgl, col=c(grey(.2),2:6))
# Use only Al and RI plane for classification as the have most separation comparatively
# total data
n = length(fgl$type)
set.seed(1)
# total training data
nt = 200
# take nt random samples from n total data
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
# required libraries
library(textir)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
# required libraries
library(textir)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- textir.normalize(fgl[,c(1,4)])
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
rm(list=ls())
# required libraries
library(textir)
library(MASS)
n = length(fgl$type)
# total training data
nt = 200
# set seed to get the random sample
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
data(fgl)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
install.packages("textir")
install.packages("textir")
# required libraries
library(textir)
library(MASS)
# the forensic glass data
data(fgl)
n = length(fgl$type)
# total training data
nt = 200
# set seed to get the random sample
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
?normalize
??normalize
require(devtools)
install_version("textir", version = "1.8.8", repos = "http://cran.us.r-project.org")
install.packages("devtools")
install_version("textir", version = "1.8.8", repos = "http://cran.us.r-project.org")
require(devtools)
install_version("textir", version = "1.8.8", repos = "http://cran.us.r-project.org")
install_version("textir", version = "1.8-8", repos = "http://cran.us.r-project.org")
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
View(fgl)
View(fgl)
View(x)
mean(x)
sd(x)
# required libraries
#install the older version of textir
package_url <- "https://cran.r-project.org/src/contrib/Archive/textir/textir_1.8-8.tar.gz"
install.packages(package_url, repos=NULL, type='source')
library(textir)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- normalize(fgl[,c(1,4)])
sessionInfo()
uninstall.packages("textir")
uninstall("textir")
# required libraries
#install the older version of textir
package_url <- "https://cran.r-project.org/src/contrib/Archive/textir/textir_1.8-8.tar.gz"
install.packages(package_url, repos=NULL, type='source')
install.packages(package_url, repos=NULL, type='source', dependencies = T)
install.packages('slam')
install.packages(package_url, repos=NULL, type='source', dependencies = T)
install.packages(package_url, repos=NULL, type='source', dependencies = T)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
library(class)
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train], bg=nearest1, pch=21, col=grey(.9), cex=1.25)
points(x[-train], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
# total training data
nt = 199
# set seed for reproducing
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
# classification library
library(class)
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
# total training data
nt = 150
# set seed for reproducing
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
# classification library
library(class)
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
library(MASS)
# the forensic glass data
data(fgl)
#set the grid size
par(mfrow=c(3,3), mai=c(.3,.6,.1,.1))
# Use only Al and RI plane for classification as the have most separation comparatively
# total data
n = length(fgl$type)
# total training data
nt = 150
# set seed for reproducing
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
# classification library
library(class)
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
length(x[-train])
length(x[train])
length(x[train,])
length(x[-train,])
points(x[-train,], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
points(x[-train,], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train,], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train,], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
legend("topright", legend=(levels(fgl$type), fill=1:5, bty="n", cex=.75))
legend("topright", legend=levels(fgl$type), fill=1:5, bty="n", cex=.75)
length(nearest1)
# proportions of correct classification
pcorrn1 = 100 * sum(fgl$type[-train]==nearest1)/(n-nt)
pcorrn2 = 100 * sum(fgl$type[-train]==nearest5)/(n-nt)
ppcorn1
pcorrn1
pcorrn2
# total training data
nt = 200
# set seed for reproducing
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
# classification library
library(class)
# returns the classification of the train data
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train,], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train,], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
legend("topright", legend=levels(fgl$type), fill=1:5, bty="n", cex=.75)
# proportions of correct classification
pcorrn1 = 100 * sum(fgl$type[-train]==nearest1)/(n-nt)
pcorrn2 = 100 * sum(fgl$type[-train]==nearest5)/(n-nt)
pcorrn1
pcorrn2
n = length(fgl$type)
# total training data
nt = 200
# set seed for reproducing
set.seed(1)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
# classification library
library(class)
# returns the classification of the train data
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train,], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train,], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
legend("topright", legend=levels(fgl$type), fill=1:5, bty="n", cex=.75)
# proportions of correct classification
pcorrn1 = 100 * sum(fgl$type[-train]==nearest1)/(n-nt)
pcorrn2 = 100 * sum(fgl$type[-train]==nearest5)/(n-nt)
pcorrn1
pcorrn2
n = length(fgl$type)
# total training data
nt = 200
# set seed for reproducing
set.seed(7)
# take nt random samples from n total data without replacement
train <- sample(n, nt)
# As the units of Al(col 4) and Ri(col 1) are different we standarize them
# we use the normalize function
x <- scale(fgl[,c(1,4)])
x[1:3,]
# classification library
library(class)
# returns the classification of the train data
nearest1 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train],k=1)
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
# create dataframe with both
data.frame(fgl$type[-train],nearest1,nearest5)
# plots
par(mfrow=c(1,2))
# Plot for k=1
plot(x[train,], col=fgl$type[train], cex=.8, main = "K=1")
points(x[-train,], bg=nearest1, pch=25, col=grey(.9), cex=1.25)
# plot for k = 5
plot(x[train,],col=fgl$type[train], cex=.8, main="K=5")
points(x[-train,], bg=nearest5, pch=25, col=grey(.9), cex=1.25)
legend("topright", legend=levels(fgl$type), fill=1:5, bty="n", cex=.75)
# proportions of correct classification
pcorrn1 = 100 * sum(fgl$type[-train]==nearest1)/(n-nt)
pcorrn2 = 100 * sum(fgl$type[-train]==nearest5)/(n-nt)
pcorrn1
pcorrn2
# cross-validation to choose the k value
pcorr = dim(10)
for (k in 1:10){
pred = knn.cv(x,fgl$type,k)
pcorr[k] = 100*sum(fgl$type==pred)/n
}
pcorr
nearest5 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
nearest6 <- knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=6)
pcorrn6 = 100 * sum(fgl$type[-train]==nearest6)/(n-nt)
pcorrn6
pcorr = dim(10)
for (k in 1:10){
pred = knn.cv(x,fgl$type,k)
pcorr[k] = 100*sum(fgl$type==pred)/n
}
pcorr
# using all glasses
x <- scale(fgl[,c[1:9]])
x
# using all glasses
x <- scale(fgl[,c(1:9)])
x
nearest1 = knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=1)
nearest5 = knn(train=x[train,], test=x[-train,], cl=fgl$type[train], k=5)
data.frame(fgl$type[-train],nearest1,nearest5)
pcorrn1 = 100*sum(fgl$type[-train]==nearest1)/(n-nt)
pcorrn1
pcorrn2 = 100*sum(fgl$type[-train]==nearest5)/(n-nt)
pcorrn2
# cross-validation
pcoor = dim(10)
for (k in 1:10){
pred = knn.cv(x,fgl$type==pred)/n
pcorr[k] = 100*sum(fgl$type==pred)/n
}
for (k in 1:10){
pred = knn.cv(x,fgl$type,k)
pcorr[k] = 100*sum(fgl$type==pred)/n
}
pcorr
rm(ls=list())
rm(list=ls())
iris
library(tree)
install.packages('tree')
library(tree)
tree_1 <- tree(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)
tree_1
plot(tree_1)
text(tree_1, cex=.75)
tree_1 <- tree(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)
tree_1
plot(tree_1)
text(tree_1, cex=.75)
library(MASS)
library(tree)
iris
iristree <- tree(Species~., data=iris)
rm(list=ls())
library(MASS)
library(tree)
iris
iristree <- tree(Species~., data=iris)
# display the tree
iristree
plot(iristree)
plot(iristree,col=8)
text(iristree,digits=2)
# display the summary of the data
summary(iristree)
levels(iris$Species)
# remove the splits with identical classes on node 12 and 7
irisremove = snip.tree(iristree, nodes=c(7,12))
plot(irisremove)
text(irisremove)
rm(list= ls())
setwd("C:\All\CS\ML\Prostate Decision Tree")
setwd("C:\\All\\CS\\ML\\Prostate Decision Tree")
data = read.csv("prostate.csv")
View(data)
library(tree)
ptree <- tree(lcavol~., data=data, mincut=1)
ptree
plot(ptree,col=8)
text(ptree, digits=2)
# pruning
p_tree <- prune.tree(ptree,k=1.7)
plot(p_tree)
p_tree
# different k-value
p_tree_2 <- prune.tree(ptree, k=2)
plot(p_tree_2)
p_tree_2
# k=3
p_tree_3 <- prune.tree(ptree, k=3)
p_tree_3
plot(p_tree_3)
p_tre <- prune.tree(ptree)
p_tre
plot(p_tre)
# k=3
p_tree_3 <- prune.tree(ptree, k=8)
p_tree_3
plot(p_tree_3)
p_tre <- prune.tree(ptree)
p_tre
plot(p_tre)
# get the best 3
pb_three <- prune.tree(ptree,best = 3)
plot(pb_three)
pb_three
text(pb_three)
# v-crossfoldvalidate
set.seed(2)
cvpst <- cv.tree(ptree)
cvpst
cvpst$size
cvpst$dev
cvpst <- cv.tree(ptree,K=10)
cvpst$size
cvpst$dev
plot(cvpst, pch=21, bg=8, type="p", cex=1.5, ylim=c(65,100))
# 3 seems to be the best
best <- prune.tree(ptree,best=3)
plot(ptree)
plot(best)
text(best)
# lcp and lpsa are used
plot(data[,c("lcp", "lpsa")], cex=0.2*exp(data$lcavol))
# draw the line for the first separation
abline(v=.261624,col=4,lwd=2)
lines(x=c(-2,.261624), y=c(2.30257,2.30257), col=4,lwd=2)
rm(list= ls())
data = read.csv("prostate.csv")
